/**
    * @license
    * Copyright 2023 Google LLC. All Rights Reserved.
    * Licensed under the Apache License, Version 2.0 (the "License");
    * you may not use this file except in compliance with the License.
    * You may obtain a copy of the License at
    *
    * http://www.apache.org/licenses/LICENSE-2.0
    *
    * Unless required by applicable law or agreed to in writing, software
    * distributed under the License is distributed on an "AS IS" BASIS,
    * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    * See the License for the specific language governing permissions and
    * limitations under the License.
    * =============================================================================
    */
(function (global, factory) {
    typeof exports === 'object' && typeof module !== 'undefined' ? factory(exports, require('@tensorflow-models/body-segmentation'), require('@tensorflow/tfjs-converter'), require('@tensorflow/tfjs-core')) :
    typeof define === 'function' && define.amd ? define(['exports', '@tensorflow-models/body-segmentation', '@tensorflow/tfjs-converter', '@tensorflow/tfjs-core'], factory) :
    (global = typeof globalThis !== 'undefined' ? globalThis : global || self, factory(global.depthEstimation = {}, global.bodySegmentation, global.tf, global.tf));
})(this, (function (exports, bodySegmentation, tfconv, tf) { 'use strict';

    function _interopNamespace(e) {
        if (e && e.__esModule) return e;
        var n = Object.create(null);
        if (e) {
            Object.keys(e).forEach(function (k) {
                if (k !== 'default') {
                    var d = Object.getOwnPropertyDescriptor(e, k);
                    Object.defineProperty(n, k, d.get ? d : {
                        enumerable: true,
                        get: function () { return e[k]; }
                    });
                }
            });
        }
        n["default"] = e;
        return Object.freeze(n);
    }

    var bodySegmentation__namespace = /*#__PURE__*/_interopNamespace(bodySegmentation);
    var tfconv__namespace = /*#__PURE__*/_interopNamespace(tfconv);
    var tf__namespace = /*#__PURE__*/_interopNamespace(tf);

    /*! *****************************************************************************
    Copyright (c) Microsoft Corporation.

    Permission to use, copy, modify, and/or distribute this software for any
    purpose with or without fee is hereby granted.

    THE SOFTWARE IS PROVIDED "AS IS" AND THE AUTHOR DISCLAIMS ALL WARRANTIES WITH
    REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF MERCHANTABILITY
    AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY SPECIAL, DIRECT,
    INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES WHATSOEVER RESULTING FROM
    LOSS OF USE, DATA OR PROFITS, WHETHER IN AN ACTION OF CONTRACT, NEGLIGENCE OR
    OTHER TORTIOUS ACTION, ARISING OUT OF OR IN CONNECTION WITH THE USE OR
    PERFORMANCE OF THIS SOFTWARE.
    ***************************************************************************** */

    var __assign = function() {
        __assign = Object.assign || function __assign(t) {
            for (var s, i = 1, n = arguments.length; i < n; i++) {
                s = arguments[i];
                for (var p in s) if (Object.prototype.hasOwnProperty.call(s, p)) t[p] = s[p];
            }
            return t;
        };
        return __assign.apply(this, arguments);
    };

    function __awaiter(thisArg, _arguments, P, generator) {
        function adopt(value) { return value instanceof P ? value : new P(function (resolve) { resolve(value); }); }
        return new (P || (P = Promise))(function (resolve, reject) {
            function fulfilled(value) { try { step(generator.next(value)); } catch (e) { reject(e); } }
            function rejected(value) { try { step(generator["throw"](value)); } catch (e) { reject(e); } }
            function step(result) { result.done ? resolve(result.value) : adopt(result.value).then(fulfilled, rejected); }
            step((generator = generator.apply(thisArg, _arguments || [])).next());
        });
    }

    function __generator(thisArg, body) {
        var _ = { label: 0, sent: function() { if (t[0] & 1) throw t[1]; return t[1]; }, trys: [], ops: [] }, f, y, t, g;
        return g = { next: verb(0), "throw": verb(1), "return": verb(2) }, typeof Symbol === "function" && (g[Symbol.iterator] = function() { return this; }), g;
        function verb(n) { return function (v) { return step([n, v]); }; }
        function step(op) {
            if (f) throw new TypeError("Generator is already executing.");
            while (_) try {
                if (f = 1, y && (t = op[0] & 2 ? y["return"] : op[0] ? y["throw"] || ((t = y["return"]) && t.call(y), 0) : y.next) && !(t = t.call(y, op[1])).done) return t;
                if (y = 0, t) op = [op[0] & 2, t.value];
                switch (op[0]) {
                    case 0: case 1: t = op; break;
                    case 4: _.label++; return { value: op[1], done: false };
                    case 5: _.label++; y = op[1]; op = [0]; continue;
                    case 7: op = _.ops.pop(); _.trys.pop(); continue;
                    default:
                        if (!(t = _.trys, t = t.length > 0 && t[t.length - 1]) && (op[0] === 6 || op[0] === 2)) { _ = 0; continue; }
                        if (op[0] === 3 && (!t || (op[1] > t[0] && op[1] < t[3]))) { _.label = op[1]; break; }
                        if (op[0] === 6 && _.label < t[1]) { _.label = t[1]; t = op; break; }
                        if (t && _.label < t[2]) { _.label = t[2]; _.ops.push(op); break; }
                        if (t[2]) _.ops.pop();
                        _.trys.pop(); continue;
                }
                op = body.call(thisArg, _);
            } catch (e) { op = [6, e]; y = 0; } finally { f = t = 0; }
            if (op[0] & 5) throw op[1]; return { value: op[0] ? op[1] : void 0, done: true };
        }
    }

    /**
     * @license
     * Copyright 2021 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * https://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    /**
     * Transform value ranges.
     * @param fromMin Min of original value range.
     * @param fromMax Max of original value range.
     * @param toMin New min of transformed value range.
     * @param toMax New max of transformed value range.
     */
    function transformValueRange(fromMin, fromMax, toMin, toMax) {
        var fromRange = fromMax - fromMin;
        var toRange = toMax - toMin;
        if (fromRange === 0) {
            throw new Error("Original min and max are both " + fromMin + ", range cannot be 0.");
        }
        var scale = toRange / fromRange;
        var offset = toMin - fromMin * scale;
        return { scale: scale, offset: offset };
    }
    /**
     * Convert an image to an image tensor representation.
     *
     * The image tensor has a shape [1, height, width, colorChannel].
     *
     * @param input An image, video frame, or image tensor.
     */
    function toImageTensor(input) {
        return input instanceof tf__namespace.Tensor ? input : tf__namespace.browser.fromPixels(input);
    }

    function toNumber(value) {
        return value instanceof SVGAnimatedLength ? value.baseVal.value : value;
    }
    /**
     * Converts input image to an HTMLCanvasElement. Note that converting
     * back from the output of this function to imageData or a Tensor will be lossy
     * due to premultiplied alpha color values. For more details please reference:
     * https://developer.mozilla.org/en-US/docs/Web/API/CanvasRenderingContext2D/putImageData#data_loss_due_to_browser_optimization
     * @param image Input image.
     *
     * @returns Converted HTMLCanvasElement.
     */
    function toHTMLCanvasElementLossy(image) {
        return __awaiter(this, void 0, void 0, function () {
            var canvas, ctx;
            return __generator(this, function (_a) {
                switch (_a.label) {
                    case 0:
                        canvas = document.createElement('canvas');
                        if (!(image instanceof tf__namespace.Tensor)) return [3 /*break*/, 2];
                        return [4 /*yield*/, tf__namespace.browser.toPixels(image, canvas)];
                    case 1:
                        _a.sent();
                        return [3 /*break*/, 3];
                    case 2:
                        canvas.width = toNumber(image.width);
                        canvas.height = toNumber(image.height);
                        ctx = canvas.getContext('2d');
                        if (image instanceof ImageData) {
                            ctx.putImageData(image, 0, 0);
                        }
                        else {
                            ctx.drawImage(image, 0, 0);
                        }
                        _a.label = 3;
                    case 3: return [2 /*return*/, canvas];
                }
            });
        });
    }

    /**
     * A calculator for changing the background of an image to black based on
     * results of segmentation returned by bodySegmentation package.
     * @param image Input image.
     * @param segmentation Segmentation mask of foreground/background.
     *
     * @returns Masked image.
     */
    function segmentForeground(image, segmentation) {
        if (tf__namespace.getBackend() === 'webgl') {
            // Same as implementation in the else case but in one custom shader on GPU.
            return segmentForegroundWebGL(image, segmentation);
        }
        // tf.tidy is unnecessary since this function is called within a tf.tidy
        // context.
        var _a = image.shape, height = _a[0], width = _a[1], channels = _a[2];
        // Extract red channel which stores probability
        var probability = tf__namespace.slice(segmentation, 0, [height, width, 1]);
        // Round to make a binary mask that can be multiplied with image.
        var binaryMask = tf__namespace.round(probability);
        // Make the same shape as input image.
        var imageBinaryMask = tf__namespace.tile(binaryMask, [1, 1, channels]);
        return tf__namespace.mul(image, imageBinaryMask);
    }
    function segmentForegroundWebGL(image, segmentation) {
        var program = {
            variableNames: ['image', 'segmentation'],
            outputShape: image.shape,
            userCode: "\n  void main() {\n      ivec3 coords = getOutputCoords();\n      int height = coords[0];\n      int width = coords[1];\n      int channel = coords[2];\n      float value = getImage(height, width, channel);\n      float foregroundProbability = getSegmentation(height, width, 0);\n      setOutput(foregroundProbability >= 0.5 ? value : 0.0);\n    }\n"
        };
        var webglBackend = tf__namespace.backend();
        var outputTensorInfo = webglBackend.compileAndRun(program, [image, segmentation]);
        return tf__namespace.engine().makeTensorFromDataId(outputTensorInfo.dataId, outputTensorInfo.shape, outputTensorInfo.dtype);
    }

    var DEFAULT_AR_PORTRAIT_DEPTH_MODEL_URL = 'https://tfhub.dev/tensorflow/tfjs-model/ar_portrait_depth/1';
    var DEFAULT_AR_PORTRAIT_DEPTH_MODEL_CONFIG = {
        depthModelUrl: DEFAULT_AR_PORTRAIT_DEPTH_MODEL_URL,
    };
    var DEFAULT_AR_PORTRAIT_DEPTH_ESTIMATION_CONFIG = {
        flipHorizontal: false,
    };

    /**
     * @license
     * Copyright 2022 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * https://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    function validateModelConfig(modelConfig) {
        if (modelConfig == null) {
            return __assign({}, DEFAULT_AR_PORTRAIT_DEPTH_MODEL_CONFIG);
        }
        var config = __assign({}, modelConfig);
        if (config.depthModelUrl == null) {
            config.depthModelUrl = DEFAULT_AR_PORTRAIT_DEPTH_MODEL_CONFIG.depthModelUrl;
        }
        return config;
    }
    function validateEstimationConfig(estimationConfig) {
        if (estimationConfig == null || estimationConfig.minDepth == null ||
            estimationConfig.maxDepth == null) {
            throw new Error('An estimation config with ' +
                'minDepth and maxDepth set must be provided.');
        }
        if (estimationConfig.minDepth > estimationConfig.maxDepth) {
            throw new Error('minDepth must be <= maxDepth.');
        }
        var config = __assign({}, estimationConfig);
        if (config.flipHorizontal == null) {
            config.flipHorizontal =
                DEFAULT_AR_PORTRAIT_DEPTH_ESTIMATION_CONFIG.flipHorizontal;
        }
        return config;
    }

    var ARPortraitDepthMap = /** @class */ (function () {
        function ARPortraitDepthMap(depthTensor) {
            this.depthTensor = depthTensor;
        }
        ARPortraitDepthMap.prototype.toCanvasImageSource = function () {
            return __awaiter(this, void 0, void 0, function () {
                return __generator(this, function (_a) {
                    return [2 /*return*/, toHTMLCanvasElementLossy(this.depthTensor)];
                });
            });
        };
        ARPortraitDepthMap.prototype.toArray = function () {
            return __awaiter(this, void 0, void 0, function () {
                return __generator(this, function (_a) {
                    return [2 /*return*/, this.depthTensor.arraySync()];
                });
            });
        };
        ARPortraitDepthMap.prototype.toTensor = function () {
            return __awaiter(this, void 0, void 0, function () {
                return __generator(this, function (_a) {
                    return [2 /*return*/, this.depthTensor];
                });
            });
        };
        ARPortraitDepthMap.prototype.getUnderlyingType = function () {
            return 'tensor';
        };
        return ARPortraitDepthMap;
    }());
    var TRANSFORM_INPUT_IMAGE = transformValueRange(0, 255, 0, 1);
    var PORTRAIT_HEIGHT = 256;
    var PORTRAIT_WIDTH = 192;
    /**
     * ARPortraitDepth estimator class.
     */
    var ARPortraitDepthEstimator = /** @class */ (function () {
        function ARPortraitDepthEstimator(segmenter, estimatorModel) {
            this.segmenter = segmenter;
            this.estimatorModel = estimatorModel;
        }
        /**
         * Estimates depth for an image or video frame.
         *
         * It returns a depth map of the same number of values as input pixels.
         *
         * @param image
         * ImageData|HTMLImageElement|HTMLCanvasElement|HTMLVideoElement The input
         * image to feed through the network.
         *
         * @param config Optional.
         *       minDepth: The minimum depth value for the model to map to 0. Any
         * smaller depth values will also get mapped to 0.
         *
         *       maxDepth`: The maximum depth value for the model to map to 1. Any
         * larger depth values will also get mapped to 1.
         *
         *       flipHorizontal: Optional. Default to false. When image data comes
         * from camera, the result has to flip horizontally.
         *
         * @return `DepthMap`.
         */
        ARPortraitDepthEstimator.prototype.estimateDepth = function (image, estimationConfig) {
            return __awaiter(this, void 0, void 0, function () {
                var config, image3d, _a, height, width, segmentations, segmentation, segmentationTensor, depthTensor;
                var _this = this;
                return __generator(this, function (_b) {
                    switch (_b.label) {
                        case 0:
                            config = validateEstimationConfig(estimationConfig);
                            if (image == null) {
                                this.reset();
                                return [2 /*return*/, null];
                            }
                            image3d = tf__namespace.tidy(function () {
                                var imageTensor = tf__namespace.cast(toImageTensor(image), 'float32');
                                if (config.flipHorizontal) {
                                    var batchAxis = 0;
                                    imageTensor = tf__namespace.squeeze(tf__namespace.image.flipLeftRight(
                                    // tslint:disable-next-line: no-unnecessary-type-assertion
                                    tf__namespace.expandDims(imageTensor, batchAxis)), [batchAxis]);
                                }
                                return imageTensor;
                            });
                            _a = image3d.shape, height = _a[0], width = _a[1];
                            return [4 /*yield*/, this.segmenter.segmentPeople(image3d)];
                        case 1:
                            segmentations = _b.sent();
                            segmentation = segmentations[0];
                            return [4 /*yield*/, segmentation.mask.toTensor()];
                        case 2:
                            segmentationTensor = _b.sent();
                            depthTensor = tf__namespace.tidy(function () {
                                var maskedImage = segmentForeground(image3d, segmentationTensor);
                                segmentationTensor.dispose();
                                // Normalizes the values from [0, 255] to [0, 1], same ranged used
                                // during training.
                                var imageNormalized = tf__namespace.add(tf__namespace.mul(maskedImage, TRANSFORM_INPUT_IMAGE.scale), 
                                // tslint:disable-next-line: no-unnecessary-type-assertion
                                TRANSFORM_INPUT_IMAGE.offset);
                                var imageResized = tf__namespace.image.resizeBilinear(imageNormalized, [PORTRAIT_HEIGHT, PORTRAIT_WIDTH]);
                                // Shape after expansion is [1, height, width, 3].
                                var batchInput = tf__namespace.expandDims(imageResized);
                                // Depth prediction (ouput shape is [1, height, width, 1]).
                                var depth4D = _this.estimatorModel.predict(batchInput);
                                // Normalize to user requirements.
                                var depthTransform = transformValueRange(config.minDepth, config.maxDepth, 0, 1);
                                // depth4D is roughly in [0,2] range, so half the scale factor to put it
                                // in [0,1] range.
                                var scale = depthTransform.scale / 2;
                                var result = 
                                // tslint:disable-next-line: no-unnecessary-type-assertion
                                tf__namespace.add(tf__namespace.mul(depth4D, scale), depthTransform.offset);
                                // Keep in [0,1] range.
                                var resultClipped = tf__namespace.clipByValue(result, 0, 1);
                                // Rescale to original input size.
                                var resultResized = tf__namespace.image.resizeBilinear(resultClipped, [height, width]);
                                // Remove channel dimension.
                                // tslint:disable-next-line: no-unnecessary-type-assertion
                                var resultSqueezed = tf__namespace.squeeze(resultResized, [0, 3]);
                                return resultSqueezed;
                            });
                            image3d.dispose();
                            return [2 /*return*/, new ARPortraitDepthMap(depthTensor)];
                    }
                });
            });
        };
        ARPortraitDepthEstimator.prototype.dispose = function () {
            this.segmenter.dispose();
            this.estimatorModel.dispose();
        };
        ARPortraitDepthEstimator.prototype.reset = function () {
            this.segmenter.reset();
        };
        return ARPortraitDepthEstimator;
    }());
    /**
     * Loads the ARPortraitDepth model.
     *
     * @param modelConfig ModelConfig object that contains parameters for
     * the ARPortraitDepth loading process. Please find more details of each
     * parameters in the documentation of the `ARPortraitDepthModelConfig`
     * interface.
     */
    function load(modelConfig) {
        return __awaiter(this, void 0, void 0, function () {
            var config, depthModelFromTFHub, depthModel, segmenter;
            return __generator(this, function (_a) {
                switch (_a.label) {
                    case 0:
                        config = validateModelConfig(modelConfig);
                        depthModelFromTFHub = typeof config.depthModelUrl === 'string' &&
                            (config.depthModelUrl.indexOf('https://tfhub.dev') > -1);
                        return [4 /*yield*/, tfconv__namespace.loadGraphModel(config.depthModelUrl, { fromTFHub: depthModelFromTFHub })];
                    case 1:
                        depthModel = _a.sent();
                        return [4 /*yield*/, bodySegmentation__namespace.createSegmenter(bodySegmentation__namespace.SupportedModels.MediaPipeSelfieSegmentation, { runtime: 'tfjs', modelUrl: config.segmentationModelUrl })];
                    case 2:
                        segmenter = _a.sent();
                        return [2 /*return*/, new ARPortraitDepthEstimator(segmenter, depthModel)];
                }
            });
        });
    }

    exports.SupportedModels = void 0;
    (function (SupportedModels) {
        SupportedModels["ARPortraitDepth"] = "ARPortraitDepth";
    })(exports.SupportedModels || (exports.SupportedModels = {}));

    /**
     * @license
     * Copyright 2022 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * https://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    /**
     * Create a depth estimator instance.
     *
     * @param model The name of the pipeline to load.
     * @param modelConfig The configuration for the pipeline to load.
     */
    function createEstimator(model, modelConfig) {
        return __awaiter(this, void 0, void 0, function () {
            return __generator(this, function (_a) {
                switch (model) {
                    case exports.SupportedModels.ARPortraitDepth:
                        return [2 /*return*/, load(modelConfig)];
                    default:
                        throw new Error(model + " is not a supported model name.");
                }
            });
        });
    }

    exports.createEstimator = createEstimator;

    Object.defineProperty(exports, '__esModule', { value: true });

}));
